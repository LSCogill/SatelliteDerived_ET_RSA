---
# Credit to Abri de Buys - SAEON, Jonkershoek Principal Investigator
title: "SAEON EC Post Processing routine"
author: "Abri de Buys"
Researcher: "LS Cogill"
date of usage: "28/3/2024"
output:
  pdf_document: default
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```

#Welcome to the draft SAEON EC Post Processing workflow. This forms part of the data management SOP for SAEON eddy covariance data and includes the major steps listed below

#1) Evaluate time series completeness - What percentage of data are NA due to rainfall, instrument downtime, etc.\
#2) Despiking according to plausibility thresholds (Brummer, pers.comm. 2019) - followed by summary stats on gaps\
#3) Selection of quality criteria - Fundamental research grade or FLUXNET grade (Foken 2012) - followed by summary stats on gaps\
#4) Preparation of a ReddyProc input file (calculations, dates & headers)\
#5) Replace long runs of equal NEE and LE and H values by NA\
#6) Time stamp formatting and plotting of data before gapfilling (fingerprint, daily mean and diurnal cycle plots)\
#7) Setting uStar thresholds manually\
#8) uStar filtering & Gap filling\

First, make sure you have the following R - packages installed:

```{r Required packages}
#install.packages("REddyProc")
#install.packages("dplyr")
#install.packages("xts")
#install.packages("lubridate")
#install.packages("openair")
#install.packages("ggplot2")
#install.packages("gridExtra")
```

...and open their libraries:

```{r Libraries, warning=FALSE, message=FALSE}
library(REddyProc)
library(dplyr)
library(xts)
library(lubridate)
library(openair)
library(ggplot2)
library(gridExtra)

#For a description of REddyProc and what it does, see here: 
vignette('useCase')
```

REddyproc postprocessing involves three major functions: uStar filtering, Gap filling and Flux partitioning.

However, there are several steps we need to take before we postprocess data using REddyProc. Our data from Easyflux need to be checked for duplicates and missing data. Then we need to run plausibility checks and discard outliers. We also have to filter the data according to the Foken quality scores.\

In the example below we will use some data from the Jonkershoek EC station. The input data file is a .csv file generated by opening the CSI format .dat file (straight off the logger) and saving it as a .csv.\

Note that of course your file paths and file names will vary.\

Let's start: 

To start we read in the data and set all non-numeric data to NA.\

```{r Read in data, warning=FALSE, message=FALSE}
setwd("C:\\Users\\Liam\\OneDrive - Stellenbosch University\\ET_Data\\RP_FormatSets")
d<-read.csv("__Site Code Here.csv",skip=0, sep = ",")

d[d == "NaN"] <- NA
d[d == "NAN"] <- NA
```

Next we identify and remove duplicate rows...\

```{r Remove duplicate rows, results='hide'}
duplicated(d)        #Identify
d[duplicated(d), ]   #View duplicates
d[!duplicated(d), ]  #View dataframe without duplicates

```

Next we generate some date and time columns for possible later use. Date formats often cause problems, so this shotgun approach helps to avoid hassles later. This also helps us in the next step where we check if our data set is complete

```{r Dates, results='hide'}
# Timestamp/date format if needed
d$date<- as.POSIXct(as.character(d$TIMESTAMP), format="%Y/%m/%d %k:%M")
#d$date<- as.POSIXct(as.character(d$TIMESTAMP), format="%Y/%m/%d:%H")
d$datetime <- strftime(d$date, format = "%Y-%m-%d %k:%M")  

```

Next we need to ensure we have a continuous data set, in other words, that every half hour between the start and end time is present in the time series, even if there's no actual data\
To do this, we make up a complete time series using our data set's start and end date and time. We then ask R to join this to our actual time series by comparing the date/time column.\
Where gaps are found, R inserts the missing time stamps and fills the rest of the row with NAs.\

Then we generate some date and time columns for possible later use. Date formats often cause problems, so this shotgun approach helps to avoid hassles later. This also helps us in the next step where we check if our data set is complete

```{r complete time series, results='hide'}
ts <- seq.POSIXt(as.POSIXct("2020-01-29 12:30",'%Y-%m-%d %H:%M', tz="Africa/Johannesburg"), as.POSIXct("2023-10-24 09:00",'%Y-%m-%d %H:%M', tz="Africa/Johannesburg"), by="30 min")  #Insert the start and end TIMESTAMPS of your raw data. This creates a TIMESTAMP column without any missing hours

ts <- format.POSIXct(ts,'%Y-%m-%d %k:%M', tz="Africa/Johannesburg")  #Ensure the TIMESTAMP format in the ts is the same as in your raw data

df <- data.frame(timestamp=ts) #Change your ts into a dataframe before you can join it to your raw data

colnames(df) <- c("datetime") #Change your time series dataframe's column heading to "datetime" to match the raw data column heading

d <- left_join(df,d, by = "datetime") ##Join the the time series and your raw data by the "datetime" column

d$year <- strftime(d$datetime, format = "%Y")
d$yearmon <- strftime(d$datetime, format = "%Y%m")
d$DT<- as.POSIXlt(d$datetime)
d$doy<- strftime(d$datetime, format = "%j")
d$doy<-as.numeric(d$doy)
d$hour<-strftime(d$datetime, format = "%H")
d$min<-strftime(d$datetime, format = "%M")
d$HM<- as.numeric(d$hour) + (as.numeric(d$min)/60)
d$dt <- as.Date(d$datetime)

x<-subset(d, select = c(year, doy, HM, DT, hour, min, HM, dt, datetime, TIMESTAMP, yearmon))

```

Now we have a continuous data set of half hourly data without duplicates or missing rows. 

Let's start looking at the state of our data and evaluate what we have to work with.

Firstly, let's check how many NAs we have in the important CO2, Latent Heat and Sensible heat flux columns.\
Gaps (NAs) can be introduced by factors like instrument obstructions (rainfall), instrument downtime (malfunctions, calibrations, power supply problems), etc.\
We need to see to what extent these issues affect our data.\

```{r, Time series completeness, warning=FALSE, message=FALSE, results='hide'}
#1) Evaluate time series completeness (% NAs due to rainfall, instrument downtime, etc.)
#%NAs in time series
(sum(is.na(d$LE))/nrow(d))*100       #Latent heat flux

```

Now that we have an idea of how much data are missing, we can make a decision whether it's worth analyzing the data further.\

In the next step we clean the data by excluding unreasonable/outlier values in the CO2, Latent Heat, Sensible heat flux and other columns (you can add as many as you wish) using predetermined thresholds. Some of these thresholds are estimated by observing the data and identifying obvious outliers. Your ecosystem may exhibit more or less flux than the thresholds below, so pay attention to what's "normal" for your site. Limits may also be determined by sensor limitations.

```{r Plausibility thresholds, warning=FALSE, message=FALSE, results='hide'}
#2) Exclude data that are outside of plausable limits
#Plausibility thresholds (Brummer, 2019 - pers.comm.) are run to exclude data that are unreasonable
d$LE<- ifelse(d$LE < -200 | d$LE > 1000, NA, d$LE)

```

Let's see how much data we have left after outliers have been removed.

```{r Summary, warning=FALSE, message=FALSE, results='hide'}
#Summary stats (after implausible spikes are thrown out)
#%NAs in time series after plausibility filter
(sum(is.na(d$LE))/nrow(d))*100       #Latent heat flux

```

Let's plot the most important flux time series to see what the data looks like.

```{r First plots, warning=FALSE, message=FALSE}
#First plots of the data after despiking
dle<-ggplot(data = d, aes(x = dt, y = LE)) + geom_point(size = 0.1) + labs( y= "LE (W/m2)")+ 
  scale_x_date(date_breaks = "1 month", date_labels = "%Y/%m") + theme_bw() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 6))  

#Plot the dirty data(pre quality filter)
A<- grid.arrange(dle,ncol=1,nrow=1, top ="Plausibility filtered")

```

At this point a decision needs to be made regarding data quality. EasyFlux assigns quality scores to EC data (following Foken, 2012.) Briefly, data with a quality score of 1-3 is considered research grade, 4-6 is good usable data, \> 6 is rubbish. For our purposes ie. getting an idea of ecological processes, data with a quality score of 6 or better is acceptable. Data of this quality are also defined as "general use", accepted by Fluxnet\

See: Foken et al. (2012) "Eddy Covariance: A Practical Guide to Measurement and Data Analysis" by Aubinet, Vesala, and Papale from Springer. Chapter 4 titled "Corrections and Data Quality Control" is written by Foken et al.\

So, we filter data based on quality and see what's left...

```{r Quality filtering,warning=FALSE, message=FALSE, results='hide'}
#3) DECIDE ON DATA QUALITY APPROPRIATE FOR YOUR PURPOSE and filter the flux data sets to keep only data of acceptable quality
#Exclude data with Foken (2012) quality grading > 6 (general use, ie. for FLUXNET)
d$LE<- ifelse(d$LE_SSITC_TEST > "6", NA, d$LE )

#Summary stats for time series completeness after QC filters have been applied
#%NAs in time series after QC filter
(sum(is.na(d$LE))/nrow(d))*100       #Latent heat flux

```

Let's plot the quality filtered data and see what the data looks like now...

```{r Quality filtered plots, warning=FALSE, message=FALSE}
#Plot quality filtered data

le<-ggplot(data = d, aes(x = dt, y = LE)) + geom_point(size = 0.1) + labs( y= "LE (W/m2)")+ 
  scale_x_date(date_breaks = "1 month", date_labels = "%Y/%m") + theme_bw() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 6))  

#Create an object that contains all plots after plausibility thresholds and quality filters
B<-grid.arrange(le, ncol=1,nrow=1, top ="Quality flag filtered")
grid.arrange(B,ncol=1,nrow=1)
```

<br>

At this point we are ready to prep the dataset for processing with REddyProc.\

This requires a few steps to be taken including subsetting of relevant columns and calculation of certain variables\

```{r REddyProc data prep, warning=FALSE, message=FALSE, results='hide' }
#4) Preparation of a REDDYPROC input file
#Subset the columns that REddyProc needs
d<-subset(d, select = c(datetime, HM, year, doy, Fc_molar, LE, H, u_star, VPD_air, SWC_1_1_1, SWC_2_1_1, SWC_3_1_1, SWC_4_1_1, SW_IN, TS_1_1_1, TS_2_1_1, TA_1_1_1, TA_2_1_1, TA_3_1_1, RH_1_1_1, RH_2_1_1, RH_3_1_1))

#Make net radiation <0 equal to 0
d$Rn<- ifelse(d$SW_IN < 0, NA, d$SW_IN )

#Average the soil temps into a single soil temp column
d$Tsoil<- rowMeans(d[c('TS_1_1_1', 'TS_2_1_1')], na.rm=TRUE)

#Average the soil temps into a single soil temp column
d$Tair<- rowMeans(d[c('TA_1_1_1', 'TA_2_1_1', 'TA_3_1_1')], na.rm=TRUE)

#Average the soil temps into a single soil temp column
d$rH<- rowMeans(d[c('RH_1_1_1', 'RH_2_1_1', 'RH_3_1_1')], na.rm=TRUE)

#Average the soil moistures into a single soil moisture column
d$SWC<- rowMeans(d[c('SWC_1_1_1', 'SWC_2_1_1','SWC_3_1_1','SWC_4_1_1')], na.rm=TRUE)

#Make any negative VPD values zero and convert units
d$VPD_air<- ifelse(d$VPD_air < 0, NA, d$VPD_air )

#Create a dataframe containing ONLY the variables that REddyProc needs
x<-subset(d, select = c(datetime, HM, year, doy, Fc_molar, Rn, LE, H, u_star, VPD_air, SWC, Tair, rH, Tsoil))

#Prepare date format for REddyProc
z<-subset(x, select = c(year, doy, HM, Fc_molar, Rn, LE, H, u_star, VPD_air, Tsoil, SWC, Tair, rH))
colnames(z)<- c("Year", "DoY", "Hour", "NEE", "Rg", "LE", "H", "Ustar", "VPD", "Tsoil", "SWC", "Tair", "rH")

##Write out a data file that's prepped for REDDYPROC
setwd("C:\\Users\\Liam\\OneDrive - Stellenbosch University\\ET_Data\\RP_FormatSets")
write.csv(z,"__Site Code Here__rp.csv", sep="\t", col.names=T,quote=F, row.names=T, na = "NA")
```

<br> <br>

Next, open the data file you just produced in Excel and ensure the following\

1)  Data file starts with the first half hour of data of the first day - ie. 00:30\

2)  Data file ends with the last half hour of data for the last day - ie. 00:00\

3)  DELETE any extra columns that were generated during the writing out process - ie. column A\

<br>

Then we read in the data and proceed with the REddyProc routine as follows:

```{r read in data, warning=FALSE, message=FALSE, results='hide'}
setwd("C:\\Users\\Liam\\OneDrive - Stellenbosch University\\ET_Data\\RP_FormatSets")
dat<-read.csv("__Site Code Here__rp.csv",skip=0, sep = ",")

```

In the next step REddyProc filters your flux data for long runs of equal values (indicating potential errors) and replaces these with NA.

```{r Replace long runs, warning=FALSE, message=FALSE, results='hide'}
#5) Replace long runs of equal NEE values by NA
dat<- filterLongRuns(dat, "LE")

#Check what's left after long runs of identical values have been filtered out
(sum(is.na(dat$LE))/nrow(dat))*100
```

Next we do some more time formatting and column renaming as required by REddyProc

```{r Time formatting, warning=FALSE, message=FALSE, results='hide'}
#6)Time stamp formatting and plotting of data before gapfilling
#Add time stamp in POSIX time format
EddyDataWithPosix <- fConvertTimeToPosix(
  dat, 'YDH', Year = 'Year',
  Day = 'DoY',Hour = 'Hour')
```

Now we are ready to create a REddyProc object file and start plotting and processing.\
At this stage we are still looking at data that hasn't been completely processed. We have not done any uStar filtering or gap filling yet, but we have thrown away data that were obvious outliers and data that didn't meet our quality criteria. So we do expect to see a lot of gaps.\

```{r fingerprint pre uStar & gap, warning=FALSE, message=FALSE}
eddyC <- sEddyProc$new(
  'BF1_rp', EddyDataWithPosix,
  c('NEE','LE', 'H', 'VPD', 'Ustar', 'SWC', 'Rg', 'Tsoil', 'Tair', 'rH'))

#Fingerprint plots of raw Latent Heat data
#eddyC$sPlotFingerprintY('LE', Year = 2019)
eddyC$sPlotFingerprintY('LE', Year = 2020)
eddyC$sPlotFingerprintY('LE', Year = 2021)
eddyC$sPlotFingerprintY('LE', Year = 2022)
eddyC$sPlotFingerprintY('LE', Year = 2023)

```

Next we generate and save some plots so we can visualize the NEE and LE time series properly. These will appear in a folder called "plots" that is generated in your working directory. Have a look at the .pdf files in there. You can do this for interest's sake to compare to fully processed data later. Or you can skip this step...

```{r Plot half hourly fluxes, warning=FALSE, message=FALSE, results='hide'}
#Generate pdf plots with half hourly fluxes and daily means (these are automatically placed in a folder called "plots" in your working directory)
eddyC$sPlotHHFluxes('LE')
```

REddyproc can also produce monthly plots of mean diurnal fluxes. These are generated in the same "plots" folder. Again, this step is optional.

```{r Plot mean diurnal by month, warning=FALSE, message=FALSE, results='hide'}
#Generate pdf plots with diurnal flux cycle by month (these are automatically placed in a folder called "plots" in your working directory)
eddyC$sPlotDiurnalCycle('LE')
```

<br>

One of the major functions of REddyProc is uStar filtering. You can choose to let REddyProc determine uStar thresholds automatically or you can set it manually. The purpose of uStar filtering is to remove data that were recorded when turbulent mixing (represented by uStar) was too low for the EC technique to work.

We set a uStar threshold at 0.13 (you could also use 0.1). Anything below that indicates conditions where turbulence was too low.

```{r Set uStar threshold, warning=FALSE, message=FALSE, results='hide'}
#7) SETTING USTAR THRESHOLDS MANUALLY 
#--------------------------------------------------------
#0.1 to 0.13 are options. Test various choices and see what gets rid of rubbish data (Brummer, 2019 pers.com)
#If you want to set uStar thresholds manually, use the following:
uStar <- 0.13
```

Next is another major step in the REddyProc routine - Gap filling. REddyProc uses sophisticated algorithms to fill data gaps based on adjacent and ancillary data. Read up more here: \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*

```{r Gap filling and fingerprint post uStar & filling, warning=FALSE, message=FALSE}

#8) GAP FILLING AFTER MANUAL USTAR FILTERING
#--------------------
# Fill a dataset that has been filtered With manually set uStar threshold
eddyC$sMDSGapFillAfterUstar('LE', uStarTh = uStar)

#Plot fingerprint plots 
grep("LE_uStar_f$",names(eddyC$sExportResults()), value = TRUE)
eddyC$sPlotFingerprintY('LE_uStar_f', Year = 2020)
eddyC$sPlotFingerprintY('LE_uStar_f', Year = 2021)
eddyC$sPlotFingerprintY('LE_uStar_f', Year = 2022)
eddyC$sPlotFingerprintY('LE_uStar_f', Year = 2023)

```

To visualize the quality controlled, gap filled data using the standard REddyProc output functions we generate another set of graphs in the "plots" folder (previously set up in your working directory)\

```{r Plot processed data, warning=FALSE, message=FALSE}
#Generate pdf plots with half hourly fluxes and daily means (these are automatically placed in a folder called "plots" in your working directory)
eddyC$sPlotHHFluxes('LE_uStar_f')

#Generate pdf plots with diurnal flux cycle by month (these are automatically placed in a folder called "plots" in your working directory)
eddyC$sPlotDiurnalCycle('LE_uStar_f')
```

The last step in the REddyProc routine is to write out a file with fully processed data in the FLUXNET format.\

This is done as follows:

```{r Producing a Fluxnet data set, warning=FALSE, message=FALSE, results='hide'}
#Export post processed results to a .txt
FilledEddyData <- eddyC$sExportResults()
CombinedData <- cbind(EddyDataWithPosix, FilledEddyData)

setwd("C:\\Users\\Liam\\OneDrive - Stellenbosch University\\ET_Data\\Gapfilled_Field_Data")
write.csv(CombinedData,"__Site Code Here_gf.csv", sep="\t", col.names=T,quote=F, row.names=T, na = "NA")

#This gives you gap filled, quality controlled data. 
########################
```

<br>

<br>

Next we can draw some basic graphs of our data to see if there are any obvious problems.\

```{r Read data in, warning=FALSE, message=FALSE, results='hide'}
#PLOTTING
#--------------------------------------------------------------------------------------------------------------------
setwd("C:\\Users\\Liam\\OneDrive - Stellenbosch University\\ET_Data\\Gapfilled_Field_Data")  
dat<-read.csv("__Site Code Here__gf.csv",skip=0)

#Generate a date column in a suitable format
dat$date<- as.POSIXct(as.character(dat$DateTime), format="%Y-%m-%d %k:%M")
dat$d<-as.Date(dat$date)

#Set all Fluxnet gap codes to NA
dat[dat == -9999] <- NA

#Check level of gaps filled
(sum(is.na(dat$LE_uStar_f))/nrow(dat))*100       #Latent heat flux
```

Let's generate some plots!

```{r Plot, warning=FALSE, message=FALSE}

cle<-ggplot(data = dat, aes(x = d, y = LE_uStar_f)) + geom_point(size = 0.1) + labs( y= "LE (W/m2)")  + scale_x_date(date_breaks = "1 month", date_labels = "%Y%m") + theme(axis.text.x=element_text(angle = 90, vjust = 0.5))

#You can arrange these in any order using grid.arrange
grid.arrange(cle, ncol=1,nrow=1, top ="Gap Filled Data")

```


################################ 

#END OF PROJECT \################################
